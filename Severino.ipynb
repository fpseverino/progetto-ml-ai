{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fpseverino/progetto-ml-ai/blob/main/Severino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dLf35CffNFTo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import train_test_split, HalvingGridSearchCV\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classe Task\n",
        "Viene definita una classe `Task`, che viene inizializzata a partire da un `DataFrame` e ha come attributi `X`, `y` e le loro suddivisioni per il training e il testing.\n",
        "\n",
        "La classe contiene anche dei metodi per testare il `Task` con uno specifico classificatore e per scalare l'`X` con uno specifico scaler."
      ],
      "metadata": {
        "id": "vyzZnl7sx9mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Task:\n",
        "  def __init__(self, df):\n",
        "    self.X = df.drop(\"Label\", axis = 1)\n",
        "    self.X = self.X.drop(\"Task\", axis = 1)\n",
        "    self.X = self.X.drop(\"Id\", axis = 1)\n",
        "    self.y = df[\"Label\"]\n",
        "\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "  def test(self, clf):\n",
        "    clf.fit(self.X_train, self.y_train)\n",
        "    return clf.score(self.X_test, self.y_test)\n",
        "\n",
        "  def scale(self, sc):\n",
        "    sc.fit(self.X_train)\n",
        "    self.X_train = sc.transform(self.X_train)\n",
        "    self.X_test = sc.transform(self.X_test)"
      ],
      "metadata": {
        "id": "9IkqpWEFh13q"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Di seguito vengono letti i `csv` dei Task già ripuliti e vengono salvati come oggetti `Task` all'interno dell'array `tasks`."
      ],
      "metadata": {
        "id": "dJilUGyXydfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = []\n",
        "for i in range(11, 22):\n",
        "  df = pd.read_csv(\"Task_{}_clean.csv\".format(i))\n",
        "  task = Task(df)\n",
        "  tasks.append(task)"
      ],
      "metadata": {
        "id": "S_RmKKO-Qz4H"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificatore `AdaBoost`\n",
        "Un classificatore AdaBoost è un meta-stimatore che inizia inserendo un classificatore nel set di dati originale e quindi adatta copie aggiuntive del classificatore nello stesso set di dati, ma dove i pesi delle istanze classificate in modo errato vengono regolati in modo tale che i classificatori successivi si concentrino maggiormente sui casi difficili.\n",
        "\n",
        "Viene eseguito un primo test sui task normali e successivamente un altro test sui task scalati per mezzo di un `MinMaxScaler`."
      ],
      "metadata": {
        "id": "XS_RmX3-yppQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = AdaBoostClassifier(n_estimators = 100, random_state = 0)\n",
        "\n",
        "scores = []\n",
        "for task in tasks:\n",
        "  scores.append(task.test(clf))\n",
        "print(\"score:\", np.mean(scores))\n",
        "\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "scaled_tasks = copy.deepcopy(tasks)\n",
        "for task in scaled_tasks:\n",
        "  task.scale(sc)\n",
        "\n",
        "scaled_scores = []\n",
        "for task in scaled_tasks:\n",
        "  scaled_scores.append(task.test(clf))\n",
        "print(\"Scaled score:\", np.mean(scaled_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na_GntyGVI-2",
        "outputId": "2064651a-5a17-42fa-ec37-a13f669cd5ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score: 0.7904040404040404\n",
            "Scaled score: 0.8257575757575758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificatore Gradient Boosting\n",
        "Questo algoritmo costruisce un modello additivo in modo graduale; consente l'ottimizzazione di funzioni di perdita differenziabili arbitrarie. In ogni fase, `n_classes_` alberi di regressione sono adattati al gradiente negativo della funzione di perdita, ad es. perdita di log binaria o multiclasse.\n",
        "\n",
        "Viene eseguito un primo test sui task normali e successivamente un altro test sui task scalati per mezzo di un `MinMaxScaler`."
      ],
      "metadata": {
        "id": "aScFCu9iz8w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = GradientBoostingClassifier(n_estimators = 500, learning_rate = 0.01, random_state = 0)\n",
        "\n",
        "scores = []\n",
        "for task in tasks:\n",
        "  scores.append(task.test(clf))\n",
        "print(\"score:\", np.mean(scores))\n",
        "\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "scaled_tasks = copy.deepcopy(tasks)\n",
        "for task in scaled_tasks:\n",
        "  task.scale(sc)\n",
        "\n",
        "scaled_scores = []\n",
        "for task in scaled_tasks:\n",
        "  scaled_scores.append(task.test(clf))\n",
        "print(\"Scaled score:\", np.mean(scaled_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "722TDyLXVr4P",
        "outputId": "4f2ddc15-2979-4a9e-a634-b051777b0485"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score: 0.8762626262626263\n",
            "Scaled score: 0.8712121212121212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Albero di classificazione Gradient Boosting basato sugli istogrammi\n",
        "Questo stimatore ha il supporto nativo per i valori mancanti (`NaN`). Durante l'addestramento, il coltivatore di alberi impara ad ogni punto di divisione se i campioni con valori mancanti devono andare al figlio sinistro o destro, in base al potenziale guadagno. Durante la previsione, i campioni con valori mancanti vengono assegnati di conseguenza al figlio sinistro o destro. Se durante l'addestramento non sono stati rilevati valori mancanti per una determinata funzionalità, i campioni con valori mancanti vengono mappati a qualunque figlio abbia il maggior numero di campioni.\n",
        "\n",
        "Viene eseguito un primo test sui task normali e successivamente un altro test sui task scalati per mezzo di un `MinMaxScaler`."
      ],
      "metadata": {
        "id": "W8OlNRpf0nxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = HistGradientBoostingClassifier(random_state = 0)\n",
        "\n",
        "scores = []\n",
        "for task in tasks:\n",
        "  scores.append(task.test(clf))\n",
        "print(\"score:\", np.mean(scores))\n",
        "\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "scaled_tasks = copy.deepcopy(tasks)\n",
        "for task in scaled_tasks:\n",
        "  task.scale(sc)\n",
        "\n",
        "scaled_scores = []\n",
        "for task in scaled_tasks:\n",
        "  scaled_scores.append(task.test(clf))\n",
        "print(\"Scaled score:\", np.mean(scaled_scores))"
      ],
      "metadata": {
        "id": "gIejO21GhzxD",
        "outputId": "fdb6b6a8-43d9-45b7-8a7b-b7af948781a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score: 0.8661616161616162\n",
            "Scaled score: 0.8661616161616162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificatore Percettrone multi-strato\n",
        "Vengono scalati i task tramite uno `StandardScaler`, in seguito viene eseguita una ricerca sui parametri forniti, e infine viene testato su tutti i task un `MLPCLassifier` con i parametri migliori."
      ],
      "metadata": {
        "id": "zWQoreTm1JnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = StandardScaler()\n",
        "scaled_tasks = copy.deepcopy(tasks)\n",
        "for task in scaled_tasks:\n",
        "  task.scale(sc)\n",
        "\n",
        "parametri_mlp = {\n",
        "  'hidden_layer_sizes': [(150, 100, 50), (120, 80, 40), (100, 66, 33)],\n",
        "  'solver': ['lbfgs', 'sgd', 'adam'],\n",
        "  'alpha': [0.0001, 0.1, 100],\n",
        "  'max_iter': [100, 200, 300]\n",
        "}\n",
        "grid_mlp = HalvingGridSearchCV(MLPClassifier(), parametri_mlp, n_jobs = -1, cv = 3)\n",
        "\n",
        "scores = []\n",
        "for task in scaled_tasks:\n",
        "  grid_mlp.fit(task.X_train, task.y_train)\n",
        "\n",
        "  clf = MLPClassifier(\n",
        "    hidden_layer_sizes = grid_mlp.best_params_[\"hidden_layer_sizes\"],\n",
        "    solver = grid_mlp.best_params_[\"solver\"],\n",
        "    alpha = grid_mlp.best_params_[\"alpha\"],\n",
        "    max_iter = grid_mlp.best_params_[\"max_iter\"],\n",
        "    random_state = 0\n",
        "  )\n",
        "\n",
        "  scores.append(task.test(clf))\n",
        "print(\"score:\", np.mean(scores))"
      ],
      "metadata": {
        "id": "x2qjfpb8qUva",
        "outputId": "a1a61330-4283-4a70-fc98-d99029284847",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score: 0.7979797979797979\n"
          ]
        }
      ]
    }
  ]
}